#!/bin/bash
# Example usage of webspider tool

echo "WebSpider - Example Usage Scenarios"
echo "==================================="

# Example 1: Discover directory structure with rate limiting
echo ""
echo "1. Discover PDF files from a hypothetical university site:"
echo "./webspider.exe -url \"https://university.edu/publications/\" \\"
echo "  -discover-only \\"
echo "  -rate 0.5 \\"
echo "  -accept \"\.(pdf|doc|docx)$\" \\"
echo "  -depth 4 \\"
echo "  -save-list \"academic-papers.txt\" \\"
echo "  -verbose"

echo ""
echo "2. Download software packages with conservative rate limiting:"
echo "./webspider.exe -url \"https://releases.example.com/\" \\"
echo "  -discover-only \\"
echo "  -rate 0.25 \\"
echo "  -accept \"\.(tar\.gz|zip|deb|rpm)$\" \\"
echo "  -depth 2 \\"
echo "  -save-list \"packages.txt\""
echo ""
echo "# After editing packages.txt to select desired files:"
echo "./webspider.exe -urls \"packages.txt\" -rate 0.5 -output \"./software\""

echo ""
echo "3. Mirror documentation while excluding assets:"
echo "./webspider.exe -url \"https://docs.example.com/\" \\"
echo "  -rate 1.5 \\"
echo "  -reject \"\.(jpg|jpeg|png|gif|svg|css|js)$\" \\"
echo "  -accept \"\.(html|htm|pdf|txt|md)$\" \\"
echo "  -depth 5 \\"
echo "  -output \"./docs-mirror\""

echo ""
echo "4. Very conservative crawling for rate-limited servers:"
echo "./webspider.exe -url \"https://sensitive-server.com/files/\" \\"
echo "  -discover-only \\"
echo "  -rate 0.1 \\"
echo "  -timeout 60s \\"
echo "  -depth 3 \\"
echo "  -user-agent \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\" \\"
echo "  -verbose"

echo ""
echo "Remember:"
echo "- Start with discovery-only mode (-discover-only)"
echo "- Use low rate limits (0.1-1.0 req/sec) for respectful crawling"
echo "- Edit the generated URL list before downloading"
echo "- Monitor with -verbose flag for debugging"